{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73609af4-f391-4b99-986a-824ef2a8f1d6",
   "metadata": {},
   "source": [
    "MNIST Number Indentifier\n",
    "\n",
    "Michael Pham, Fall 2022\n",
    "Based on math laid out by Samson Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8ef16-fec4-4f73-8b70-429ab4660282",
   "metadata": {},
   "source": [
    "Will only be using numpy for calculations, matplotlib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "52e00233-5beb-46c0-ab9d-a08402fe6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6658a-5224-4d98-99bb-b935d6626f92",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fd808-c4f8-427e-b0af-abb74bc75a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3897e8c-1190-47a2-a35b-d90daa853d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad51c9-146b-4f5d-a687-0909dda1367a",
   "metadata": {},
   "source": [
    "splitting data into testing and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8983e-2130-4d39-8073-aeef49e68bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.array(data)\n",
    "np.random.shuffle(data_matrix)\n",
    "m, n = data_matrix.shape\n",
    "\n",
    "total_n = m #adjustable\n",
    "test_n = int(total_n / 3.333)\n",
    "training_n = total_n - test_n\n",
    "\n",
    "training_set = data_matrix[0:training_n].T\n",
    "test_set = data_matrix[training_n :].T\n",
    "\n",
    "x_train = training_set[1:]/255 #input train\n",
    "y_train = training_set[0] #target train\n",
    "x_test = test_set[1:]/255 #input test\n",
    "y_test = test_set[0] #target test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45628a-6850-4149-9269-6565e1a86c33",
   "metadata": {},
   "source": [
    "machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2fa62-c49e-490f-bf7b-8623f8b2b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    #initialize parameters\n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.rand(10, 784) - 0.5\n",
    "        self.b1 = np.random.rand(10,1) - 0.5\n",
    "        self.w2 = np.random.rand(10, 10) - 0.5\n",
    "        self.b2 = np.random.rand(10,1) - 0.5\n",
    "    \n",
    "    \n",
    "    #ReLu function\n",
    "    ReLu = lambda self, z: np.maximum(0, z)\n",
    "    #softmax function\n",
    "    softmax = lambda self, z: np.exp(z) / sum(np.exp(z))\n",
    "    #derivative of ReLu function\n",
    "    derivative_ReLu = lambda self, z: z > 0\n",
    "    \n",
    "    \n",
    "    #forward propagation\n",
    "    def forward_propagation(self, x):\n",
    "        self.z1 = self.w1.dot(x) + self.b1\n",
    "        self.a1 = self.ReLu(self.z1)\n",
    "        self.z2 = self.w2.dot(self.a1) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    #encode probabilities into 0-1\n",
    "    def hot_encode(self, y):\n",
    "        one_hot_y = np.zeros((y.size, y.max() + 1))\n",
    "        one_hot_y[np.arange(y.size), y] = 1\n",
    "\n",
    "        return one_hot_y.T\n",
    "\n",
    "    \n",
    "    #backwards propagation\n",
    "    def backwards_propagation(self, x, y):\n",
    "        one_hot_y = self.hot_encode(y)\n",
    "        m = y.size\n",
    "\n",
    "        self.dz2 = self.a2 - one_hot_y\n",
    "        self.dw2 = (1/m) * self.dz2.dot(self.a1.T)\n",
    "        self.db2 = 1/m * np.sum(self.dz2)\n",
    "        self.dz1 = (self.w2.T).dot(self.dz2) * self.derivative_ReLu(self.z1)\n",
    "        self.dw1 = 1/m * self.dz1.dot(x.T)\n",
    "        self.db1 = 1/m * np.sum(self.dz1)\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    #update parameters\n",
    "    def update_parameters(self,alpha):\n",
    "        self.w1 = self.w1 - alpha * self.dw1\n",
    "        self.b1 = self.b1 - alpha * self.db1\n",
    "        self.w2 = self.w2 - alpha * self.dw2\n",
    "        self.b2 = self.b2 - alpha * self.db2\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    #convert probabilities into prediction\n",
    "    get_predictions = lambda self: np.argmax(self.a2, 0)\n",
    "    #calculates how accurate predicted y is vs real y\n",
    "    get_accuracy = lambda self, predictions, y: np.sum(predictions == y)/y.size\n",
    "\n",
    "    \n",
    "    #makes prediction from inputs\n",
    "    def make_predictions(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        predictions = self.get_predictions()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    #plots training error vs test error graph\n",
    "    def generalization_graph(self, x_graph, train_graph, test_graph):\n",
    "        plt.plot(x_graph, train_graph, label = \"training\")\n",
    "        plt.plot(x_graph, test_graph, label = \"test\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "        return\n",
    "    \n",
    "    \n",
    "    #performs gradient descent\n",
    "    def gradient_descent(self, x, y, x_test, y_test, iterations, alpha):\n",
    "        x_graph = []\n",
    "        train_graph = []\n",
    "        test_graph = []\n",
    "        for i in range(iterations):\n",
    "            self.forward_propagation(x)\n",
    "            self.backwards_propagation(x, y)\n",
    "            self.update_parameters(alpha)\n",
    "            \n",
    "            predictions = self.get_predictions()\n",
    "            training_accuracy = self.get_accuracy(predictions, y)\n",
    "            training_error = 1-training_accuracy\n",
    "            \n",
    "            test_predictions = self.make_predictions(x_test)\n",
    "            testing_accuracy = self.get_accuracy(test_predictions, y_test)\n",
    "            testing_error = 1-testing_accuracy\n",
    "                             \n",
    "            x_graph.append(i)\n",
    "            train_graph.append(training_error)\n",
    "            test_graph.append(testing_error)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Iteration: \", i)\n",
    "                print(\"training set accuracy\", training_accuracy)\n",
    "                print(\"test set accuracy\", testing_accuracy)\n",
    "                \n",
    "        self.generalization_graph(x_graph, train_graph, test_graph)\n",
    "                \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bbbeb1-ea85-4a9e-8352-e9e65930b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.gradient_descent(x_train, y_train, x_test, y_test, 2000, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f82c3-549b-4446-a820-fc9c1d1547cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
